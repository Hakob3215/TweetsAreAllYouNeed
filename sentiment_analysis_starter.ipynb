{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment-140 Classification\n",
    "\n",
    "In this notebook, we will explore the application of transformers for sentiment analysis of different tweets. Our goal is to classify tweets to have either positive or negative sentiment. We will first consider the performance of a pre-trained BERT model, finetuned on the Sentiment-140 dataset, and later we'll develop and train our own transformer architecture from scratch. Let's dive right in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from data_utils.SentimentDataset import SentimentDataset\n",
    "\n",
    "# this automatically reloads the libraries so you can update them dynamically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Data Preparation](#loading-our-data)\n",
    "2. [Fine-tuning a Pre-trained BERT Model](#fine-tuning-bert)\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "Let's start by preparing our dataset. We have implemented the dataset class for you in `data_utils/SentimentDataset.py`, feel free to check the implementation. However, before we can use the `SentimentDataset` class to create our train data and test data objects, we need to pre-process the raw data. \n",
    "\n",
    "You can download raw data from [here](https://www.kaggle.com/datasets/kazanova/sentiment140). If you examine the raw data file, you can see that there is a lot of redundant information such as the time of each tweet or usernames. For our sentiment analysis, we simply need the tweets and ground truth labels. To preprocess the data file, simply paste the original CSV file in the root project directory without changing the file name and run the script `preprocess_dataset.py`. This should create a new CSV file called `dataset.csv`. (The reason why we do not include the data files on the GitHub repository is that they are simply too big.) Then, you're ready to create a dataset and dataloader below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SentimentDataset('dataset.csv', training_set=True)\n",
    "test_data = SentimentDataset('dataset.csv', training_set=False)\n",
    "\n",
    "# train_subset_indices = np.random.choice(len(train_data), size=1000, replace=False)\n",
    "# train_sampler = SubsetRandomSampler(train_subset_indices)\n",
    "\n",
    "# test_subset_indices = np.random.choice(len(test_data), size=100, replace=False)\n",
    "# test_sampler = SubsetRandomSampler(test_subset_indices)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=32, sampler=train_sampler)\n",
    "# test_loader = DataLoader(test_data, batch_size=32, sampler=test_sampler)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample tweet from the dataset and print its corresponding ground truth label. Label 1 corresponds to positive sentiment, while label 0 stands for negative sentiment. You can run the cell below multiple times to see different tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, labels = next(iter(train_loader))\n",
    "print(\"Sample tweet: \", tweets[0])\n",
    "print(\"Ground truth label: \", labels[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT\n",
    "\n",
    "First, let's examine the performance of a pre-trained BERT model on the sentiment analysis task. BERT (original paper [here](https://arxiv.org/abs/1810.04805)) is a language representation model released by Google in 2018 that uses a Bidirectional Transformer architecture (shown below). \n",
    "\n",
    "![BERT](BERT.png)\n",
    "\n",
    "It can be easily fine-tuned on downstream tasks such as sentiment analysis by appending a classification layer to the original BERT architecture. This is already done automatically by importing the `BertForSequenceClassification` from the Hugging Face library called `transformers`. The newly added classification layer is untrained (as you'll see in the warning that appears when you run the following cell) and so we have to fine-tune on the Sentiment-140 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "pretraineemb_size = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "MAX_SEQ_LENGTH = 512 \n",
    "VOCAB_SIZE = 30522\n",
    "N_LAYERS = 12\n",
    "N_HEADS = 12\n",
    "EMB_SIZE = 768\n",
    "INTERMEDIATE_SIZE = EMB_SIZE * 4\n",
    "DROPOUT = 0.1\n",
    "N_CLASSES = 2\n",
    "LAYER_NORM_EPS = 1e-12\n",
    "PAD_TOKEN_ID = 103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a `preprocess` function that takes in the tweets and tokenizes them using the WordPiece algorithm, so that they can be process by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweets):\n",
    "\n",
    "    encoded_batch = tokenizer.batch_encode_plus(\n",
    "        tweets,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_batch['input_ids']\n",
    "    attention_masks = encoded_batch['attention_mask']\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a training and evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "          num_epochs):\n",
    "\n",
    "    # Place model on device\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Use tqdm to display a progress bar during training\n",
    "        with tqdm(total=len(train_loader),\n",
    "                  desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
    "                  position=0,\n",
    "                  leave=True) as pbar:\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "\n",
    "                input_ids, attention_masks = preprocess(inputs)\n",
    "                # Move inputs and labels to device\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the logits and loss\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "                    \n",
    "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
    "        print(\n",
    "            f'Test set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
    "        )\n",
    "        acc_history.append(accuracy)\n",
    "\n",
    "    return loss_history, acc_history\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "\n",
    "            input_ids, attention_masks = preprocess(inputs)\n",
    "            # Move inputs and labels to device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute the logits and loss\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(outputs.logits, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    # Compute the average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are running the model on CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finetune all the weights in the BERT model (including the pre-trained weights and the newly initialized classifier weights). An alternative approach would be to freeze the pre-trained weights and only fine-tune the classifier weights. You can do that by passing in `model.classifier.parameters()` to the optimizer instead of passing in all the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(pretraineemb_size.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history, acc_history = train(pretraineemb_size, train_loader, test_loader, optimizer, criterion, device, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot our training loss and test accuracy history that documents the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss')\n",
    "plt.title('BERT Fine-tuning, Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('BERT Fine-tuning, Accuracy history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our own Transformer architecture from scratch\n",
    "\n",
    "That was fun, right? But maybe slightly too easy, don't you think? Let's try to implement our own transformer architecture now, without the help of pre-trained models. We will base our architecture off of the original paper that introduced transformers, [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are a type of neural network architecture primarily used in the field of natural language processing (NLP). Unlike previous models that processed inputs sequentially (like RNNs), transformers process whole sequences of data in parallel, which significantly speeds up training.\n",
    "\n",
    "### Why Important\n",
    "\n",
    "They have been highly successful in a variety of NLP tasks like translation, text summarization, and sentiment analysis due to their ability to handle long-range dependencies in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of Transformers\n",
    "\n",
    "1. Input Embeddings\n",
    "2. Positional Encoding\n",
    "3. Multi-Head Attention\n",
    "4. Layer Normalization and Residual Connections\n",
    "5. Feed-Forward Networks\n",
    "6. Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input Embeddings\n",
    "\n",
    "Converts tokens (words) into vectors of continuous numbers that the model can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of Embeddings\n",
    "\n",
    "Embeddings are vector representations of text, where words with **similar meanings** have** similar representations**. These vectors are learned and adjusted during the training of the model to optimize performance on a specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Embeddings\n",
    "\n",
    "Embeddings capture the semantic properties of words, which means they convert the words/tokens into a form that a neural network can work with. This allows the model to **understand and process language** by **quantifying the similarities between different words or phrases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings in Transformers\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - **Process**: Text input is split into tokens (words or subwords), which are then converted into numerical IDs that correspond to entries in the embedding table.\n",
    "   - **Role in Model**: This step is critical because it translates human-readable text into a format that can be mathematically manipulated by the model.\n",
    "\n",
    "2. **Lookup Table**:\n",
    "   - **Process**: Each token ID is used to retrieve its corresponding embedding vector from an embedding matrix (or table) that the model learns during training.\n",
    "   - **Role in Model**: This embedding matrix acts as the foundational input layer of the transformer, providing a dense and informative representation for each token.\n",
    "\n",
    "3. **Dimensionality**:\n",
    "   - **Process**: Embeddings are usually vectors of fixed size (e.g., 256, 512 dimensions), regardless of the vocabulary size.\n",
    "   - **Role in Model**: Higher dimensions generally capture more detailed semantic information about each token, but also increase model complexity and computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "\n",
    "If we only turn words into embeddings, the model won't know the order of them in a sentence. We need to somehow preserve the positional info.\n",
    "\n",
    "- **Concept**: Since transformers process words in parallel rather than sequentially, positional encodings are added to embeddings to give the model information about the order of words in a sentence.\n",
    "- **Why Necessary**: Helps the model understand how the position of a word in a sentence affects its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Adjustments\n",
    "\n",
    "**Learning Process**:\n",
    "- Embeddings are not static; they are adjusted during the training process to minimize the model's prediction error. This learning happens through backpropagation, which adjusts the embeddings to better capture the relationships and contexts in which words appear.\n",
    "\n",
    "**Impact on Performance**:\n",
    "- The quality of embeddings significantly affects the model's performance. Better embeddings lead to a better understanding of language nuances, such as synonyms, antonyms, and different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Vocabulary size and embedding dimension\n",
    "vocab_size = 1000  # Number of unique tokens\n",
    "embedding_dim = 64  # Size of each embedding vector\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "# Example token IDs\n",
    "token_ids = torch.LongTensor([10, 200, 500])\n",
    "\n",
    "# Get embeddings for token IDs\n",
    "embeddings = embedding_layer(token_ids)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example initializes an embedding layer with random values which then can be trained to learn meaningful representations through a task like sentiment analysis or machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Now, let's implement the `Embeddings` class. This class will be responsible for converting tokens into dense vectors that the model can understand. We will use PyTorch's `nn.Embedding` module to create the embedding layer.\n",
    "\n",
    "1. token embedding: vocab -> embedding (take a look at the `padding_idx` argument of `nn.Embedding`).\n",
    "2. positional embedding: position -> embedding\n",
    "3. add them together\n",
    "4. layer normalization\n",
    "5. dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, pad_token_id, max_seq_length, layer_norm_eps=1e-12, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Initialize word embeddings\n",
    "        self.token_embedding_table = None\n",
    "        self.position_embedding_table = None\n",
    "        self.ln = None\n",
    "        self.dropout = None\n",
    "\n",
    "        # sets self.position_ids to a tensor containing values from 0 to max_seq_length, reshaped to (1, max_seq_length)\n",
    "        # register_buffer makes it so that this does NOT get updated during training\n",
    "        self.register_buffer(\"position_ids\", torch.arange(max_seq_length).expand((1, -1)))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        pass\n",
    "\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention\n",
    "\n",
    "Multi-head attention consists of several attention mechanisms (heads) running in parallel. Each head computes an attention score known as \"Scaled Dot-Product Attention,\" which determines how much each element in a sequence should pay attention to every other element. Combining multiple heads allows the model to capture various aspects of semantic relationships in different subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Multi-Head Attention\n",
    "\n",
    "By using multiple attention heads, transformers can:\n",
    "- Capture a richer diversity of relationships within the data.\n",
    "- Focus on different parts of the sentence simultaneously.\n",
    "- Improve the ability of the model to focus on relevant parts of the input for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of Multi-Head Attention\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: The basic attention mechanism computes the dot products of the query with all keys, divides each by the square root of the dimensionality (to stabilize gradients), applies a softmax function to obtain weights on the values, and finally returns an output vector.\n",
    "\n",
    "2. **Multiple Heads**: Each head performs attention independently, allowing the model to focus on different features across different positions in the input sequence.\n",
    "\n",
    "3. **Concatenation and Final Linear Transformation**: Outputs from individual heads are concatenated and passed through a final linear layer to combine the different learned aspects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, n_heads, dropout=0.1, layer_norm_eps=1e-12):\n",
    "        '''\n",
    "        Args:\n",
    "            emb_size (int): The size of the input embeddings\n",
    "            n_heads (int): The number of attention heads\n",
    "            dropout (float): The dropout probability\n",
    "            layer_norm_eps (float): The epsilon value for layer normalization\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert emb_size % n_heads == 0, \"emb_size must be divisible by n_heads\"\n",
    "\n",
    "        # TODO: initialize head size, query, key, value, dropout, final_linear, and layer norm\n",
    "        # you want to use nn.Linear, nn.Dropout, nn.LayerNorm\n",
    "        # query, key, value, final_linear should map from emb_size to emb_size\n",
    "        self.head_size = 0\n",
    "        self.query = None \n",
    "        self.key = None\n",
    "        self.value = None\n",
    "        self.dropout = None\n",
    "\n",
    "        self.final_linear = None\n",
    "        self.ln = None\n",
    "\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        '''\n",
    "        Args:\n",
    "            emb (torch.Tensor): The input embeddings with shape (B, T, C)\n",
    "            att_mask (torch.Tensor): The attention mask with shape (B, T)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output embeddings with shape (B, T, C)\n",
    "\n",
    "        1. You want to obtain the query (Q), key (K), and value (V) tensors from the input embeddings\n",
    "        2. Compute weights: \n",
    "            a. find dot product of Q and K \n",
    "            b. divide by the square root of the head size\n",
    "            c. apply the attention mask\n",
    "            d. apply softmax\n",
    "        3. Compute attention: multiply weights by V\n",
    "        4. Feed the attention through the final linear layer\n",
    "        5. Apply dropout, residual connection, and layer normalization\n",
    "\n",
    "        '''\n",
    "        B, T, C = emb.shape  # batch size, sequence length, embedding size   \n",
    "    \n",
    "        # TODO: 1. get the query, key, and value tensors\n",
    "        # TODO: 2. rearrange the tensors from B x T x C to B x n_heads x T x head_size\n",
    "\n",
    "        q = \n",
    "        k = \n",
    "        v = \n",
    "        \n",
    "        # TODO: compute the weights before attention mask\n",
    "        weights =\n",
    "\n",
    "        # set the pad tokens to -inf so that they equal zero after softmax\n",
    "        if att_mask != None:\n",
    "            att_mask = (att_mask > 0).unsqueeze(1).repeat(1, att_mask.size(1), 1).unsqueeze(1)\n",
    "            weights = weights.masked_fill(att_mask == 0, float('-inf'))\n",
    "\n",
    "        # apply softmax and dropout\n",
    "        weights = \n",
    "        \n",
    "        # compute attention\n",
    "        attention = \n",
    "\n",
    "        # rearrange the attention tensor from B x n_heads x T x head_size to B x T x C\n",
    "\n",
    "        # apply final linear layer, dropout, residual connection, and layer normalization\n",
    "        out = \n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: Layer Normalization and Residual Connections\n",
    "\n",
    "- **Concept**: Techniques used within transformers to stabilize the learning process. Residual connections help in propagating gradients through deep networks without vanishing.\n",
    "- **Why Necessary**: Improves training efficiency and model performance by preventing the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Position-wise Feed-Forward Networks\n",
    "\n",
    "The position-wise feed-forward network is a standard fully connected neural network that applies the same two linear transformations to each position independently and identically. It consists of two layers:\n",
    "- The first linear transformation maps the input dimension `emb_size` to a dimension `intermediate_size` (typically much larger, e.g., 2048).\n",
    "- The second linear transformation maps it back from `intermediate_size` to `emb_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Position-wise Feed-Forward Networks\n",
    "\n",
    "This component of the transformer allows the model to consider each position separately and integrate non-linearity into the model. Each position in the encoder or decoder can be thought of as having its own little neural network that processes the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, intermediate_size, dropout=0.1, layer_norm_eps=1e-12):\n",
    "        '''\n",
    "        Args:\n",
    "            emb_size (int): The size of the input embeddings\n",
    "            intermediate_size (int): The size of the intermediate layer\n",
    "            dropout (float): The dropout probability\n",
    "            layer_norm_eps (float): The epsilon value for layer normalization\n",
    "\n",
    "        We want to create two fully connected layers mapping from emb_size to intermediate_size and back\n",
    "        '''\n",
    "\n",
    "        # TODO: initialize the two fully connected layers, gelu, dropout, and layer norm\n",
    "        super().__init__()\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "\n",
    "        # we want some non-linearity between the two layers, we are using GELU, feel free to experiment with other activations\n",
    "        self.gelu = None\n",
    "        self.dropout = None\n",
    "        self.ln = None\n",
    "\n",
    "    def forward(self, att_out):\n",
    "        '''\n",
    "        Args:\n",
    "            att_out (torch.Tensor): The output embeddings from the multi-head attention layer with shape (B, T, C)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output embeddings with shape (B, T, C)\n",
    "        \n",
    "        TODO:\n",
    "        1. Apply the first fully connected layer and non-linearity\n",
    "        2. Apply the second fully connected layer and dropout\n",
    "        3. Apply a residual connection and layer normalization\n",
    "        '''\n",
    "        \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of Transformers\n",
    "\n",
    "\n",
    "1. **Feed-Forward Networks**\n",
    "   - **Concept**: Each layer of the transformer includes a feed-forward neural network which applies further transformations to the output of the attention mechanism.\n",
    "   - **Why Necessary**: Adds additional capabilities to the transformer to modify the attention output before passing it to the next layer.\n",
    "\n",
    "2. **Output Layer**\n",
    "   - **Concept**: The final decoder outputs are passed through a linear layer followed by a softmax to predict the next word in the sequence.\n",
    "   - **Why Necessary**: Converts the complex representations formed by the transformer into understandable and usable output, like words in a sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlocks(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size, n_heads, intermediate_size, dropout=0.1, layer_norm_eps=1e-12):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, n_heads, dropout, layer_norm_eps)\n",
    "        self.ff = PositionWiseFeedForward(emb_size, intermediate_size, dropout, layer_norm_eps)\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        att_out = self.attn(emb, att_mask)\n",
    "        out = self.ff(att_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([Transformer() for layer_num in range(n_layers)])\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        for bert_layer in self.layer:\n",
    "            emb = bert_layer(emb, att_mask)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_size, emb_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, encoder_out):\n",
    "        pool_first_token = encoder_out[:, 0]\n",
    "        out = self.dense(pool_first_token)\n",
    "        out = self.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingLayer()\n",
    "        self.encoder = Encoder()\n",
    "        self.pooler = Pooler()\n",
    "\n",
    "    def forward(self, input_ids, att_mask):\n",
    "        emb = self.embedding(input_ids)\n",
    "        out = self.encoder(emb, att_mask)\n",
    "        pooled_out = self.pooler(out)\n",
    "        return out, pooled_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSequenceClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Model()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        _, pooled_out = self.model(input_ids, attention_mask)\n",
    "        pooled_out = self.dropout(pooled_out)\n",
    "        logits = self.classifier(pooled_out)\n",
    "         \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = TransformerSequenceClassification()\n",
    "\n",
    "optimizer = torch.optim.AdamW(our_model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history, acc_history = train(our_model, train_loader, test_loader, optimizer, criterion, device, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss')\n",
    "plt.title('BERT From Scratch, Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('BERT From Scratch, Accuracy history')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
