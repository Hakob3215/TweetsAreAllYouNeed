{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment-140 Classification\n",
    "\n",
    "In this notebook, we will explore the application of transformers for sentiment analysis of different tweets. Our goal is to classify tweets to have either positive or negative sentiment. We will first consider the performance of a pre-trained BERT model, finetuned on the Sentiment-140 dataset, and later we'll develop and train our own transformer architecture from scratch. Let's dive right in!\n",
    "\n",
    "First of all, we need a couple of useful libraries that you're hopefully familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from data_utils.SentimentDataset import SentimentDataset\n",
    "\n",
    "# this automatically reloads the libraries so you can update them dynamically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "Let's start by preparing our dataset. We have implemented the dataset class for you in `data_utils/SentimentDataset.py`, feel free to check the implementation. However, before we can use the `SentimentDataset` class to create our train data and test data objects, we need to pre-process the raw data. \n",
    "\n",
    "You can download raw data from [here](https://www.kaggle.com/datasets/kazanova/sentiment140). If you examine the raw data file, you can see that there is a lot of redundant information such as the time of each tweet or usernames. For our sentiment analysis, we simply need the tweets and ground truth labels. To preprocess the data file, simply paste the original CSV file in the root project directory without changing the file name and run the script `preprocess_dataset.py`. This should create a new CSV file called `dataset.csv`. (The reason why we do not include the data files on the GitHub repository is that they are simply too big.) Then, you're ready to create a dataset and dataloader below.\n",
    "\n",
    "Notice that the original dataset contains 1.6 million tweets; that is huge! Therefore, it may take forever to train on so much data, especially if you happen not to have access to a fast GPU (talk to the officers about getting access to the ACM AI server!). Therefore, in the next cell you can configure whether to use the whole dataset or just a subset of it. Note that, from our experience, running one training epoch on the whole dataset takes about 7 hours on RTX 3090..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_subset_of_data = True\n",
    "subset_percentage = 0.05 # set the percentage of original dataset size you'd like to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SentimentDataset('dataset.csv', training_set=True)\n",
    "test_data = SentimentDataset('dataset.csv', training_set=False)\n",
    "\n",
    "if use_subset_of_data:\n",
    "    train_set_len = int(subset_percentage * len(train_data))\n",
    "    test_set_len = int(subset_percentage * len(test_data))\n",
    "    \n",
    "    train_subset_indices = np.random.choice(len(train_data), size=train_set_len, replace=False)\n",
    "    train_sampler = SubsetRandomSampler(train_subset_indices)\n",
    "\n",
    "    test_subset_indices = np.random.choice(len(test_data), size=test_set_len, replace=False)\n",
    "    test_sampler = SubsetRandomSampler(test_subset_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, sampler=train_sampler)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, sampler=test_sampler)\n",
    "\n",
    "else:\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample tweet from the dataset and print its corresponding ground truth label. Label 1 corresponds to positive sentiment, while label 0 stands for negative sentiment. You can run the cell below multiple times to see different tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tweet:  Ughhh I need an umbrella \n",
      "Ground truth label:  0\n"
     ]
    }
   ],
   "source": [
    "tweets, labels = next(iter(train_loader))\n",
    "print(\"Sample tweet: \", tweets[0])\n",
    "print(\"Ground truth label: \", labels[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT\n",
    "\n",
    "First, let's examine the performance of a pre-trained BERT model on the sentiment analysis task. BERT (original paper [here](https://arxiv.org/abs/1810.04805)) is a language representation model released by Google in 2018 that uses a Bidirectional Transformer architecture.\n",
    "\n",
    "It can be easily fine-tuned on downstream tasks such as sentiment analysis by appending a classification layer to the original BERT architecture. This is already done automatically by importing the `BertForSequenceClassification` from the Hugging Face library called `transformers`. The newly added classification layer is untrained (as you'll see in the warning that appears when you run the following cell) and so we have to fine-tune on the Sentiment-140 data. We will also import a pre-trained tokenizer, which is necessary to convert inputs from natural language to a numerical representation machine learning models can understand. The tokenizer imported from Hugging Face uses WordPiece tokenization, you can learn more about the algorithm [here](https://huggingface.co/learn/nlp-course/en/chapter6/6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) \n",
    "pretrained_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a `preprocess` function that takes in the tweets and tokenizes them. The tokenizer will break down words into smaller units called subword tokens and map these words to numbers (IDs), which can be processed by the model. Furthermore, it will add special tokens to the sequence that will normalize inputs to the model.\n",
    "\n",
    "The `preprocess` function return `input_ids` which represents the tokenized input sequence and `attention_mask` which specifies to the model which tokens to attend to while computing self-attention (more on that later!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweets):\n",
    "\n",
    "    encoded_batch = tokenizer.batch_encode_plus(\n",
    "        tweets,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_batch['input_ids']\n",
    "    attention_masks = encoded_batch['attention_mask']\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different types of special tokens normally used by the BERT tokenizer. `[CLS]` token is added at the beginning of the input sequence. The final hidden state corresponding to the `[CLS]` token is typically used as the representation of the entire input sequence for classification tasks, so it will be very useful to us later for sentiment analysis. The `[SEP]` token is used to separate different segments of the input sequence. It helps the model distinguish between different parts of the input and process them accordingly. In our case, one `[SEP]` token will typically be added at the end of the tokenized input sequence (tweet). Finally, the `[PAD]` token is used for padding shorter input sequences to a fixed length, so that they can be easily processed in batches without any shape mismatch issues. With BERT, we would typically pad the input sequence to reach a total length of 512 tokens; however, since we're analyzing tweets that are by definition very short, using maximum sequence length of 128 tokens should suffice.\n",
    "\n",
    "Below, you can see what the IDs reserved for special tokens are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Token ID: 101, [SEP] Token ID: 102, [PAD] Token ID: 0\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"[CLS] Token ID: \" + str(cls_token_id) + \", [SEP] Token ID: \" + str(sep_token_id) + \", [PAD] Token ID: \" + str(pad_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a sample tweet and its corresponding tokenized representation. Notice that each input sequence starts with token ID 101 (CLS token), ends with token ID 102 (SEP token), and is padded with token ID 0 (PAD token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tweet:  @EPMorgan Don't know, just read on Chris F page that it wasn't looking good for Roxy and she might not be skating tonight  x \n",
      "\n",
      "Tokenized representation:  tensor([  101,  1030,  4958,  5302, 16998,  2123,  1005,  1056,  2113,  1010,\n",
      "         2074,  3191,  2006,  3782,  1042,  3931,  2008,  2009,  2347,  1005,\n",
      "         1056,  2559,  2204,  2005, 23682,  1998,  2016,  2453,  2025,  2022,\n",
      "        10080,  3892,  1060,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "tweets, labels = next(iter(train_loader))\n",
    "print(\"Sample tweet: \", tweets[0], '\\n')\n",
    "input_ids, att_masks = preprocess(tweets)\n",
    "print(\"Tokenized representation: \", input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a training and evaluation loop. Notice that we use the `preprocess` function in both the `train` and `evaluate` to transform the natural language input to tokenized representation. We will also collect loss and accuracy statistics during training, so that we can plot the training history later.\n",
    "\n",
    "Note that the `finetuning` parameter is used simply because the internal Hugging Face representation of the pre-trained model is slightly different from what our representation will be when we later define our own transformer model. Thus, when using the training loop for fine-tuning the pre-trained downloaded from Hugging Face, set `finetuning=True`, otherwise set it to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "          num_epochs, finetuning=False):\n",
    "\n",
    "    # Place model on device\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Use tqdm to display a progress bar during training\n",
    "        with tqdm(total=len(train_loader),\n",
    "                  desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
    "                  position=0,\n",
    "                  leave=True) as pbar:\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "\n",
    "                input_ids, attention_masks = preprocess(inputs)\n",
    "                # Move inputs and labels to device\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the logits and loss\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "                if finetuning: \n",
    "                    outputs = outputs.logits\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "                    \n",
    "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device, finetuning)\n",
    "        print(\n",
    "            f'Test set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
    "        )\n",
    "        acc_history.append(accuracy)\n",
    "\n",
    "    return loss_history, acc_history\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device, finetuning=False):\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "\n",
    "            input_ids, attention_masks = preprocess(inputs)\n",
    "            # Move inputs and labels to device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute the logits and loss\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "            if finetuning:\n",
    "                outputs = outputs.logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    # Compute the average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are running the model on CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finetune all the weights in the BERT model (including the pre-trained weights and the newly initialized classifier weights). An alternative approach would be to freeze the pre-trained weights and only fine-tune the classifier weights. You can do that by passing in `model.classifier.parameters()` to the optimizer instead of passing in all the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  17%|█▋        | 205/1200 [00:30<02:27,  6.74it/s, loss=0.214]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(pretrained_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 4\u001b[0m loss_history, acc_history \u001b[38;5;241m=\u001b[39m train(pretrained_model, train_loader, test_loader, optimizer, criterion, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, finetuning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, finetuning)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     46\u001b[0m         loss_history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     49\u001b[0m avg_loss, accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device, finetuning)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history, acc_history = train(pretrained_model, train_loader, test_loader, optimizer, criterion, device, num_epochs=3, finetuning=True)\n",
    "\n",
    "# Let's save our trained weights\n",
    "torch.save(pretrained_model.state_dict(), 'bert_fine_tuned.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot our training loss and test accuracy history that documents the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss')\n",
    "plt.title('BERT Fine-tuning, Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('BERT Fine-tuning, Accuracy history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our own Transformer architecture from scratch\n",
    "\n",
    "That was fun, right? But maybe slightly too easy, don't you think? Let's try to implement our own transformer architecture now, without the help of pre-trained models. We will base our architecture off of BERT, which will also allow us to effectively compare the performance of our own implementation with the pre-trained model. The high-level of the model architecture can be seen below.\n",
    "\n",
    "Note that, compared to illustration below, all the tensor sizes will most likely be different and specific to the hyperparameters of our model.\n",
    "\n",
    "![BERT For Sequence Classification](bert_classification.png)\n",
    "\n",
    "![BERT Encoder](bert_encoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some hyperparameters of our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128 # Maximum length of tokenized input sequence (we already covered this above!)\n",
    "vocab_size = 30522 # number of different token -> ID mappings, specific to the BERT Tokenizer\n",
    "n_layers = 12 # number of transformer layers stacked on top of each other\n",
    "n_heads = 12 # number of heads of the transformer, input sequence will be split and equally distributed to each head\n",
    "emb_size = 768 # refers to the dimensionality of the vector representations used to encode input tokens\n",
    "intermediate_size = emb_size * 4 # dimensionality of output of the Intermediate layer\n",
    "dropout = 0.1 # probability of dropping a neuron during training\n",
    "n_classes = 2 # we are classifying the sentiment of each tweet simply as POSITIVE or NEGATIVE\n",
    "layer_norm_eps = 1e-12 # value used in Layer Norm for numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will define the Embedding Layer. Embeddings are dense vector representations of tokens that allow us to position them in a high-dimensional vector space. These vector representations serve as coordinates for the tokens, enabling the model to capture their semantic relationships. The key idea is that tokens with similar meanings should be located closer together in the vector space, while tokens with dissimilar meanings should be farther apart.\n",
    "\n",
    "For each token, we will generate two types of embeddings, which are then combined:\n",
    "\n",
    "Word Embeddings: Vector representations of the tokens themselves, obtained from a lookup table with one row per token in the vocabulary.\n",
    "Positional Embeddings: These embeddings encode the position of each token in the input sequence, providing the model with a sense of the token order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, emb_size)\n",
    "        self.ln = nn.LayerNorm(emb_size, eps=layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"position_ids\", torch.arange(max_seq_length).expand((1, -1)))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        word_emb = self.word_embeddings(input_ids)\n",
    "        pos_emb = self.position_embeddings(self.position_ids)\n",
    "        emb = word_emb + pos_emb\n",
    "        emb = self.ln(emb)\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self-attention mechanism enables the model to effectively capture the contextual relationships between tokens in a sequence. This process enriches the embedding vector representation of each token by incorporating information about how it relates to and interacts with the surrounding tokens in the given context.\n",
    "\n",
    "We compute attention using the formula below, introduced in [this](https://arxiv.org/abs/1706.03762) famous paper.\n",
    "\n",
    "![Attention](attention.png)\n",
    "\n",
    "Let's define our `MultiHeadAttention` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head_size = emb_size // n_heads\n",
    "        self.query = nn.Linear(emb_size, emb_size)\n",
    "        self.key = nn.Linear(emb_size, emb_size)\n",
    "        self.value = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.final_linear = nn.Linear(emb_size, emb_size)\n",
    "        self.ln = nn.LayerNorm(emb_size, eps=layer_norm_eps)\n",
    "\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        B, T, C = emb.shape  # batch size, sequence length, embedding size   \n",
    "    \n",
    "        q = self.query(emb).view(B, T, n_heads, self.head_size).transpose(1, 2)\n",
    "        k = self.key(emb).view(B, T, n_heads, self.head_size).transpose(1, 2)\n",
    "        v = self.value(emb).view(B, T, n_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        weights = q @ k.transpose(-2, -1) * self.head_size ** -0.5\n",
    "\n",
    "        # set the pad tokens to -inf so that they equal zero after softmax\n",
    "        if att_mask != None:\n",
    "            att_mask = (att_mask > 0).unsqueeze(1).repeat(1, att_mask.size(1), 1).unsqueeze(1)\n",
    "            weights = weights.masked_fill(att_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        attention = weights @ v\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B, T, C)   \n",
    "\n",
    "        out = self.final_linear(attention)\n",
    "        out = self.dropout(out)\n",
    "        out = out + emb\n",
    "        out = self.ln(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position-wise feed-forward network is a key component in transformer layers that complements the self-attention mechanism. Its role is two-fold: 1) It increases the model's capacity by introducing additional learnable parameters, and 2) It facilitates the progressive enrichment and refinement of token representations as they pass through each encoder layer. Let's define `FeedForward` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, emb_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(emb_size, eps=layer_norm_eps)\n",
    "\n",
    "    def forward(self, att_out):\n",
    "        x = self.fc1(att_out)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + att_out\n",
    "        out = self.ln(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will simply put the `MultiHeadAttention` and `FeedForward` modules together to obtain one transformer block (as shown on the image above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.ff = FeedForward()\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        att_out = self.attn(emb, att_mask)\n",
    "        out = self.ff(att_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack `n_layers` of the `Transformer` module to create the `Encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([Transformer() for layer_num in range(n_layers)])\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        for bert_layer in self.layer:\n",
    "            emb = bert_layer(emb, att_mask)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hang on tight, we're almost there! We have successfully implemented the BERT Encoder architecture, but now we have to tailor the model to a clasification task like sentiment analysis.\n",
    "\n",
    "The output from the encoder is a matrix where each row represents an enriched vector representation for the corresponding input token, capturing contextual and positional information through the self-attention and feed-forward layers. However, for sequence classification tasks, a single vector representation summarizing the meaning of the entire input sequence is required, rather than individual token representations. To achieve this, we will only process the representation of the `[CLS]` token (always inserted at the start of our input sequence), as it is expected to contain a contextualized, high-level representation of the entire sequence and therefore is a good candidate to be pooled for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_size, emb_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, encoder_out):\n",
    "        pool_first_token = encoder_out[:, 0]\n",
    "        out = self.dense(pool_first_token)\n",
    "        out = self.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it all together by first passing our inputs through the `EmbeddingLayer`, followed by `Encoder` and `Pooler`, all defined above. To finish it off, append a dropout layer and a classifier that will reduce the dimensionality of the output to the number of target classes (2 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSequenceClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingLayer()\n",
    "        self.encoder = Encoder()\n",
    "        self.pooler = Pooler()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        emb = self.embedding(input_ids)\n",
    "        enc = self.encoder(emb, attention_mask)\n",
    "        pooled_out = self.pooler(enc)\n",
    "        pooled_out = self.dropout(pooled_out)\n",
    "        logits = self.classifier(pooled_out)\n",
    "         \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully implemented our own transformer-based classifier for sentiment analysis! Let's train it and see how it compares to the pre-trained one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = TransformerSequenceClassification()\n",
    "\n",
    "optimizer = torch.optim.AdamW(our_model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history, acc_history = train(our_model, train_loader, test_loader, optimizer, criterion, device, num_epochs=3, finetuning=False)\n",
    "\n",
    "# Let's save our trained weights\n",
    "torch.save(our_model.state_dict(), 'bert_from_scratch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss')\n",
    "plt.title('BERT From Scratch, Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('BERT From Scratch, Accuracy history')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
