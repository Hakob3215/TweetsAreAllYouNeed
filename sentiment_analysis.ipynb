{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment-140 Classification\n",
    "\n",
    "In this notebook, we will explore the application of transformers for sentiment analysis of different tweets. Our goal is to classify tweets to have either positive or negative sentiment. We will first consider the performance of a pre-trained BERT model, finetuned on the Sentiment-140 dataset, and later we'll develop and train our own transformer architecture from scratch. Let's dive right in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from data_utils.SentimentDataset import SentimentDataset\n",
    "\n",
    "# this automatically reloads the libraries so you can update them dynamically\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512 \n",
    "vocab_size = 30522\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "emb_size = 768\n",
    "intermediate_size = emb_size * 4\n",
    "dropout = 0.1\n",
    "n_classes = 2\n",
    "layer_norm_eps = 1e-12\n",
    "pad_token_id = 103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "Let's start by preparing our dataset. We have implemented the dataset class for you in `data_utils/SentimentDataset.py`, feel free to check the implementation. However, before we can use the `SentimentDataset` class to create our train data and test data objects, we need to pre-process the raw data. \n",
    "\n",
    "You can download raw data from [here](https://www.kaggle.com/datasets/kazanova/sentiment140). If you examine the raw data file, you can see that there is a lot of redundant information such as the time of each tweet or usernames. For our sentiment analysis, we simply need the tweets and ground truth labels. To preprocess the data file, simply paste the original CSV file in the root project directory without changing the file name and run the script `preprocess_dataset.py`. This should create a new CSV file called `dataset.csv`. (The reason why we do not include the data files on the GitHub repository is that they are simply too big.) Then, you're ready to create a dataset and dataloader below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SentimentDataset('dataset.csv', training_set=True)\n",
    "test_data = SentimentDataset('dataset.csv', training_set=False)\n",
    "\n",
    "# train_subset_indices = np.random.choice(len(train_data), size=1000, replace=False)\n",
    "# train_sampler = SubsetRandomSampler(train_subset_indices)\n",
    "\n",
    "# test_subset_indices = np.random.choice(len(test_data), size=100, replace=False)\n",
    "# test_sampler = SubsetRandomSampler(test_subset_indices)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=32, sampler=train_sampler)\n",
    "# test_loader = DataLoader(test_data, batch_size=32, sampler=test_sampler)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine a sample tweet from the dataset and print its corresponding ground truth label. Label 1 corresponds to positive sentiment, while label 0 stands for negative sentiment. You can run the cell below multiple times to see different tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, labels = next(iter(train_loader))\n",
    "print(\"Sample tweet: \", tweets[0])\n",
    "print(\"Ground truth label: \", labels[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT\n",
    "\n",
    "First, let's examine the performance of a pre-trained BERT model on the sentiment analysis task. BERT (original paper [here](https://arxiv.org/abs/1810.04805)) is a language representation model released by Google in 2018 that uses a Bidirectional Transformer architecture (shown below). \n",
    "\n",
    "![BERT](BERT.png)\n",
    "\n",
    "It can be easily fine-tuned on downstream tasks such as sentiment analysis by appending a classification layer to the original BERT architecture. This is already done automatically by importing the `BertForSequenceClassification` from the Hugging Face library called `transformers`. The newly added classification layer is untrained (as you'll see in the warning that appears when you run the following cell) and so we have to fine-tune on the Sentiment-140 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "pretrained_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a `preprocess` function that takes in the tweets and tokenizes them using the WordPiece algorithm, so that they can be process by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweets):\n",
    "\n",
    "    encoded_batch = tokenizer.batch_encode_plus(\n",
    "        tweets,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_batch['input_ids']\n",
    "    attention_masks = encoded_batch['attention_mask']\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a training and evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device,\n",
    "          num_epochs, finetuning=False):\n",
    "\n",
    "    # Place model on device\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        # Use tqdm to display a progress bar during training\n",
    "        with tqdm(total=len(train_loader),\n",
    "                  desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
    "                  position=0,\n",
    "                  leave=True) as pbar:\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "\n",
    "                input_ids, attention_masks = preprocess(inputs)\n",
    "                # Move inputs and labels to device\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_masks = attention_masks.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Compute the logits and loss\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "                if finetuning: \n",
    "                    outputs = outputs.logits\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "                    \n",
    "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device, finetuning)\n",
    "        print(\n",
    "            f'Test set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
    "        )\n",
    "        acc_history.append(accuracy)\n",
    "\n",
    "    return loss_history, acc_history\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device, finetuning=False):\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "\n",
    "            input_ids, attention_masks = preprocess(inputs)\n",
    "            # Move inputs and labels to device\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_masks = attention_masks.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute the logits and loss\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_masks)\n",
    "            if finetuning:\n",
    "                outputs = outputs.logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(outputs.logits, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(labels)\n",
    "\n",
    "    # Compute the average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are running the model on CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finetune all the weights in the BERT model (including the pre-trained weights and the newly initialized classifier weights). An alternative approach would be to freeze the pre-trained weights and only fine-tune the classifier weights. You can do that by passing in `model.classifier.parameters()` to the optimizer instead of passing in all the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history, acc_history = train(pretrained_model, train_loader, test_loader, optimizer, criterion, device, num_epochs=3, finetuning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot our training loss and test accuracy history that documents the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss')\n",
    "plt.title('BERT Fine-tuning, Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('BERT Fine-tuning, Accuracy history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our own Transformer architecture from scratch\n",
    "\n",
    "That was fun, right? But maybe slightly too easy, don't you think? Let's try to implement our own transformer architecture now, without the help of pre-trained models. We will base our architecture off of the original paper that introduced transformers, [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, emb_size)\n",
    "        self.ln = nn.LayerNorm(emb_size, eps=layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"position_ids\", torch.arange(max_seq_length).expand((1, -1)))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        word_emb = self.word_embeddings(input_ids)\n",
    "        pos_emb = self.position_embeddings(self.position_ids)\n",
    "\n",
    "        emb = word_emb + pos_emb\n",
    "        emb = self.ln(emb)\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head_size = emb_size // n_heads\n",
    "        self.query = nn.Linear(emb_size, emb_size)\n",
    "        self.key = nn.Linear(emb_size, emb_size)\n",
    "        self.value = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.final_linear = nn.Linear(emb_size, emb_size)\n",
    "        self.ln = nn.LayerNorm(emb_size, eps=layer_norm_eps)\n",
    "\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        B, T, C = emb.shape  # batch size, sequence length, embedding size   \n",
    "    \n",
    "        q = self.query(emb).view(B, T, n_heads, self.head_size).transpose(1, 2)\n",
    "        k = self.key(emb).view(B, T, n_heads, self.head_size).transpose(1, 2)\n",
    "        v = self.value(emb).view(B, T, n_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        weights = q @ k.transpose(-2, -1) * self.head_size ** -0.5\n",
    "\n",
    "        # set the pad tokens to -inf so that they equal zero after softmax\n",
    "        if att_mask != None:\n",
    "            att_mask = (att_mask > 0).unsqueeze(1).repeat(1, att_mask.size(1), 1).unsqueeze(1)\n",
    "            weights = weights.masked_fill(att_mask == 0, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        attention = weights @ v\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B, T, C)   \n",
    "\n",
    "        out = self.final_linear(attention)\n",
    "        out = self.dropout(out)\n",
    "        out = out + emb\n",
    "        out = self.ln(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_size, intermediate_size)\n",
    "        self.fc2 = nn.Linear(intermediate_size, emb_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(emb_size, eps=layer_norm_eps)\n",
    "\n",
    "    def forward(self, att_out):\n",
    "        x = self.fc1(att_out)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + att_out\n",
    "        out = self.ln(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.ff = PositionWiseFeedForward()\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        att_out = self.attn(emb, att_mask)\n",
    "        out = self.ff(att_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([Transformer() for layer_num in range(n_layers)])\n",
    "\n",
    "    def forward(self, emb, att_mask):\n",
    "        for bert_layer in self.layer:\n",
    "            emb = bert_layer(emb, att_mask)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(emb_size, emb_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, encoder_out):\n",
    "        pool_first_token = encoder_out[:, 0]\n",
    "        out = self.dense(pool_first_token)\n",
    "        out = self.tanh(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingLayer()\n",
    "        self.encoder = Encoder()\n",
    "        self.pooler = Pooler()\n",
    "\n",
    "    def forward(self, input_ids, att_mask):\n",
    "        emb = self.embedding(input_ids)\n",
    "        out = self.encoder(emb, att_mask)\n",
    "        pooled_out = self.pooler(out)\n",
    "        return out, pooled_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSequenceClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Model()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        _, pooled_out = self.model(input_ids, attention_mask)\n",
    "        pooled_out = self.dropout(pooled_out)\n",
    "        logits = self.classifier(pooled_out)\n",
    "         \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = TransformerSequenceClassification()\n",
    "\n",
    "optimizer = torch.optim.AdamW(our_model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history, acc_history = train(our_model, train_loader, test_loader, optimizer, criterion, device, num_epochs=3, finetuning=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss')\n",
    "plt.title('BERT From Scratch, Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('BERT From Scratch, Accuracy history')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
